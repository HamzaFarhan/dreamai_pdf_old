{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Some basic functions used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/anaconda3/envs/dreamai_pdf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from dreamai_pdf.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def pdf_img_to_np(img):\n",
    "    return np.array(img.annotated)\n",
    "\n",
    "def cid_to_char(cidx):\n",
    "    try:\n",
    "        return chr(int(re.findall(r'\\(cid\\:(\\d+)\\)',cidx)[0]) + 29)\n",
    "    except:\n",
    "        return cidx\n",
    "\n",
    "def col_clusters(data, data2=None, n_cols=3):\n",
    "    n_cols = int(n_cols)\n",
    "    kmeans = KMeans(n_clusters=n_cols, algorithm='elkan', random_state=42).fit(np.reshape(data,(-1,1)))\n",
    "    idx = np.argsort(kmeans.cluster_centers_.sum(axis=1)).tolist()\n",
    "    cols = defaultdict(list)\n",
    "    if data2 is None: data2 = data\n",
    "    for i,c in enumerate(kmeans.labels_):\n",
    "        cols[idx.index(c)].append(data2[i])\n",
    "    return cols\n",
    "\n",
    "class ColumnCounter(KElbowVisualizer):\n",
    "    def draw(self):\n",
    "        pass\n",
    "\n",
    "def get_n_cols(data, min_c=2, max_c=10, max_n_cols=3):\n",
    "    model = KMeans(algorithm='elkan', random_state=42)\n",
    "    visualizer = ColumnCounter(model, k=(min_c, max_c), metric='silhouette')\n",
    "    visualizer.fit(np.reshape(data,(-1,1)))\n",
    "    # print(np.reshape(data,(-1,1)))\n",
    "    if visualizer.elbow_value_ is None:\n",
    "        return max_n_cols\n",
    "    return min(visualizer.elbow_value_, max_n_cols)\n",
    "\n",
    "def combine_lines(txt):\n",
    "    avg_len = np.mean([len(w.split()) for w in txt])\n",
    "    txt2 = []\n",
    "    for i,w in enumerate(txt):\n",
    "        w = process_text(w)\n",
    "        if i==0:\n",
    "            txt2.append(w)\n",
    "        else:\n",
    "            if len(txt2[-1].split()) < avg_len:\n",
    "                txt2[-1]+=' '+w\n",
    "            else:\n",
    "                txt2.append(w)\n",
    "    return txt2\n",
    "\n",
    "def get_avg_gap(words, key0='top', key1='bottom'):\n",
    "    if key1 is None: key1 = key0\n",
    "    return np.mean([w[key0]-words[i-1][key1] for i,w in enumerate(words) if i>0])\n",
    "\n",
    "def get_max_gap(words, key0='top', key1='bottom'):\n",
    "    if key1 is None: key1 = key0\n",
    "    return np.max([w[key0]-words[i-1][key1] for i,w in enumerate(words) if i>0])\n",
    "\n",
    "def combine_splits(splits):\n",
    "    avg_len = np.mean([len(s) for s in splits])\n",
    "    splits2 = []\n",
    "    for i,s in enumerate(splits):\n",
    "        if i==0:\n",
    "            splits2.append(s)\n",
    "        else:\n",
    "            if len(splits2[-1]) < avg_len:\n",
    "                splits2[-1]+=s\n",
    "            else:\n",
    "                splits2.append(s)\n",
    "    return splits2\n",
    "\n",
    "def split_words(words, key0='top', key1='bottom', avg_gap=None, fill_empty=False):\n",
    "    if key1 is None: key1 = key0\n",
    "    if avg_gap is None:\n",
    "        avg_gap = np.mean([w[key0]-words[i-1][key1] for i,w in enumerate(words) if i>0])\n",
    "    splits = []\n",
    "    for i,w in enumerate(words):\n",
    "        if i==0:\n",
    "            splits.append([w])\n",
    "        else:\n",
    "            if w[key0]-words[i-1][key1] > avg_gap:\n",
    "                if fill_empty:\n",
    "                    splits.append(['*']*len(splits[-1]) + [w])\n",
    "                else:\n",
    "                    splits.append([w])\n",
    "            else:\n",
    "                splits[-1].append(w)\n",
    "    return splits\n",
    "\n",
    "def process_text(text):\n",
    "    text = cid_to_char(text)\n",
    "    text = re.sub(r\"\\uf0b7\", \" \", text)\n",
    "    text = re.sub(r\"\\(cid:\\d{0,3}\\)\", \" \", text)\n",
    "    text = re.sub(r'• ', \" \", text)\n",
    "    text = re.sub(r'● ', \" \", text)\n",
    "    return text\n",
    "\n",
    "def pdf_to_cols(data_path, max_n_cols=3, cols_list=[2,1]):\n",
    "    pdfs = resolve_data_path(data_path)\n",
    "    cols_dict = {}\n",
    "    for file in pdfs:\n",
    "        if Path(file).suffix == '.pdf':\n",
    "            try:\n",
    "                with pdfplumber.open(file) as pdf:\n",
    "                    pdf_pages = pdf.pages\n",
    "                    cols_list = cols_list + [None]*(len(pdf_pages)-len(cols_list))\n",
    "                    pdf_cols = []\n",
    "                    for page, n_cols in zip(pdf_pages, cols_list):\n",
    "                        words = page.extract_words(x_tolerance=5)\n",
    "                        if len(words) == 0:\n",
    "                            # raise Exception(f'\\nCould not extract words from pdf: {str(file)}\\nMaybe try extracting tables?')\n",
    "                            print(f'\\nCould not extract words from pdf: {str(file)}. Maybe try extracting tables?')\n",
    "                            continue\n",
    "                        word_x = [w['x0'] for w in words]\n",
    "                        if n_cols is None:\n",
    "                            try:\n",
    "                                n_cols = get_n_cols(word_x, max_n_cols=max_n_cols)\n",
    "                            except:\n",
    "                                print(f'\\nCould not find ideal number of columns for pdf: {str(file)}. Setting to 1.')\n",
    "                                n_cols = 1\n",
    "                        # if n_cols == 0:\n",
    "                            # print(file)\n",
    "                        cols = col_clusters(word_x, words, n_cols=n_cols)\n",
    "                        cols = sort_dict({k:sorted(v, key=lambda x: x['top']) for k,v in cols.items()})\n",
    "                        for k,v in cols.items():\n",
    "                            paras = []\n",
    "                            avg_gap = np.mean([w['top']-v[i-1]['top'] for i,w in enumerate(v) if i>0])\n",
    "                            for i,w in enumerate(v):\n",
    "                                txt = w['text']\n",
    "                                if i==0:\n",
    "                                    paras.append(txt)\n",
    "                                else:\n",
    "                                    if w['top']-v[i-1]['bottom'] >= avg_gap:\n",
    "                                        paras.append(txt)\n",
    "                                    else:\n",
    "                                        paras[-1]+=' '+txt.strip()\n",
    "                            paras = combine_lines(paras)                \n",
    "                            cols[k] = paras\n",
    "                        pdf_cols.append(cols)\n",
    "                    cols = defaultdict(list)\n",
    "                    for c in pdf_cols:\n",
    "                        for k,v in c.items():\n",
    "                            cols[k]+=v\n",
    "                    cols = sort_dict(cols)\n",
    "                    cols_dict[str(file)] = cols\n",
    "            except:\n",
    "                continue\n",
    "    return cols_dict\n",
    "\n",
    "def pdf_cols_to_text(pdf_cols):\n",
    "    return flatten_list([dict_values(d) for d in dict_values(pdf_cols)])\n",
    "\n",
    "def pdf_to_text(data_path, max_n_cols=3, cols_list=[2,1]):\n",
    "    pdf_cols = pdf_to_cols(data_path, max_n_cols=max_n_cols, cols_list=cols_list)\n",
    "    return pdf_cols_to_text(pdf_cols)\n",
    "\n",
    "def text_to_segments(text, labeling_model, tags=['education', 'work experience']):\n",
    "    segs = defaultdict(list)\n",
    "    for txt in text:\n",
    "        pred = tags[labeling_model(txt, tags)[0][0]]\n",
    "        segs[pred].append(txt)\n",
    "    return segs\n",
    "\n",
    "def segment_to_ners(text, tagger):\n",
    "    if is_list(text):\n",
    "        text = ' '.join(text)\n",
    "    s = Sentence(text)\n",
    "    tagger.predict(s)\n",
    "    return s\n",
    "\n",
    "def ners_to_dicts(s, search_tags=['ORG', 'DATE'], dict_keys=['COMPANY', 'DATE']):\n",
    "    tags_list = []\n",
    "    tags_dict = {}\n",
    "    for l in s.labels:\n",
    "        dp = l.data_point\n",
    "        tag = dp.tag\n",
    "        for s,k in zip(search_tags, dict_keys):\n",
    "            if tag == s:\n",
    "                if not tags_dict.get(k,None):\n",
    "                    tags_dict[k] = dp.text.strip()\n",
    "                else:\n",
    "                    tags_list.append(tags_dict)\n",
    "                    tags_dict = {k:dp.text.strip()}\n",
    "                \n",
    "    return tags_list\n",
    "\n",
    "def get_edu_dicts(edu, tagger):\n",
    "    edu = segment_to_ners(edu, tagger)\n",
    "    edu_list = ners_to_dicts(edu, search_tags=['ORG', 'DATE'], dict_keys=['INSTITUTE', 'DATE'])\n",
    "    edu_list = [d for d in edu_list if d.get('INSTITUTE', None) is not None]\n",
    "    return edu_list\n",
    "\n",
    "def get_job_dicts(job, tagger):\n",
    "    job = segment_to_ners(job, tagger)\n",
    "    job_dict = ners_to_dicts(job, search_tags=['ORG', 'DATE'], dict_keys=['COMPANY', 'DATE'])\n",
    "    job_dict = [d for d in job_dict if d.get('COMPANY', None) is not None]\n",
    "    return job_dict\n",
    "\n",
    "def get_contact_dict(text):\n",
    "    if is_list(text): text = ' '.join(text)\n",
    "    mail_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "    phone_regex = re.compile(r'[\\d]{3}[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}')\n",
    "    emails = re.findall(mail_regex, text.lower())\n",
    "    phones = re.findall(phone_regex, text.lower())\n",
    "    return {'EMAIL':emails, 'PHONE':phones}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
