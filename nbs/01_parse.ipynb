{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from dreamai_pdf.core import *\n",
    "from dreamai_pdf.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def col_clusters(data, data2=None, n_cols=3):\n",
    "    n_cols = int(n_cols)\n",
    "    kmeans = KMeans(n_clusters=n_cols, algorithm='elkan', random_state=42).fit(np.reshape(data,(-1,1)))\n",
    "    idx = np.argsort(kmeans.cluster_centers_.sum(axis=1)).tolist()\n",
    "    cols = defaultdict(list)\n",
    "    if data2 is None: data2 = data\n",
    "    for i,c in enumerate(kmeans.labels_):\n",
    "        cols[idx.index(c)].append(data2[i])\n",
    "    return cols\n",
    "\n",
    "class ColumnCounter(KElbowVisualizer):\n",
    "    def draw(self):\n",
    "        pass\n",
    "\n",
    "def get_n_cols(data, min_c=2, max_c=10, max_n_cols=3):\n",
    "    model = KMeans(algorithm='elkan', random_state=42)\n",
    "    visualizer = ColumnCounter(model, k=(min_c, max_c), metric='silhouette')\n",
    "    visualizer.fit(np.reshape(data,(-1,1)))\n",
    "    # print(np.reshape(data,(-1,1)))\n",
    "    if visualizer.elbow_value_ is None:\n",
    "        return max_n_cols\n",
    "    return min(visualizer.elbow_value_, max_n_cols)\n",
    "\n",
    "def pdf_to_cols(data_path, model=None, max_n_cols=3, cols_list=[2,1], device='cpu'):\n",
    "    pdfs = resolve_data_path(data_path)\n",
    "    cols_dict = {}\n",
    "    for file in pdfs:\n",
    "        if Path(file).suffix == '.pdf':\n",
    "            try:\n",
    "                if model is not None:\n",
    "                    cols_list = pred_cols(file, model, device=device)\n",
    "                with pdfplumber.open(file) as pdf:\n",
    "                    pdf_pages = pdf.pages\n",
    "                    cols_list = cols_list + [None]*(len(pdf_pages)-len(cols_list))\n",
    "                    pdf_cols = []\n",
    "                    for page, n_cols in zip(pdf_pages, cols_list):\n",
    "                        words = page.extract_words(x_tolerance=5)\n",
    "                        if len(words) == 0:\n",
    "                            # raise Exception(f'\\nCould not extract words from pdf: {str(file)}\\nMaybe try extracting tables?')\n",
    "                            print(f'\\nCould not extract words from pdf: {str(file)}. Maybe try extracting tables?')\n",
    "                            continue\n",
    "                        word_x = [w['x0'] for w in words]\n",
    "                        if n_cols is None:\n",
    "                            try:\n",
    "                                n_cols = get_n_cols(word_x, max_n_cols=max_n_cols)\n",
    "                            except:\n",
    "                                print(f'\\nCould not find ideal number of columns for pdf: {str(file)}. Setting to 1.')\n",
    "                                n_cols = 1\n",
    "                        # if n_cols == 0:\n",
    "                            # print(file)\n",
    "                        cols = col_clusters(word_x, words, n_cols=n_cols)\n",
    "                        cols = sort_dict({k:sorted(v, key=lambda x: x['top']) for k,v in cols.items()})\n",
    "                        for k,v in cols.items():\n",
    "                            paras = []\n",
    "                            avg_gap = np.mean([w['top']-v[i-1]['top'] for i,w in enumerate(v) if i>0])\n",
    "                            for i,w in enumerate(v):\n",
    "                                txt = w['text']\n",
    "                                if i==0:\n",
    "                                    paras.append(txt)\n",
    "                                else:\n",
    "                                    if w['top']-v[i-1]['bottom'] >= avg_gap:\n",
    "                                        paras.append(txt)\n",
    "                                    else:\n",
    "                                        paras[-1]+=' '+txt.strip()\n",
    "                            paras = combine_lines(paras)                \n",
    "                            cols[k] = paras\n",
    "                        pdf_cols.append(cols)\n",
    "                    cols = defaultdict(list)\n",
    "                    for c in pdf_cols:\n",
    "                        for k,v in c.items():\n",
    "                            cols[k]+=v\n",
    "                    cols = sort_dict(cols)\n",
    "                    cols_dict[str(file)] = cols\n",
    "            except:\n",
    "                continue\n",
    "    return cols_dict\n",
    "\n",
    "def pdf_cols_to_text(pdf_cols):\n",
    "    return flatten_list([dict_values(d) for d in dict_values(pdf_cols)])\n",
    "\n",
    "def pdf_to_text(data_path, model=None, max_n_cols=3, cols_list=[2,1], device='cpu'):\n",
    "    pdf_cols = pdf_to_cols(data_path, model=model, max_n_cols=max_n_cols, cols_list=cols_list, device=device)\n",
    "    return pdf_cols_to_text(pdf_cols)\n",
    "\n",
    "class HeadModel(nn.Module):\n",
    "    def __init__(self, pool, linear):\n",
    "        super().__init__()\n",
    "        # store_attr(self, 'pool,linear')\n",
    "        self.pool = pool\n",
    "        self.linear = linear\n",
    "    def forward(self, x, meta=None):\n",
    "        if meta is None:\n",
    "            return self.linear(self.pool(x))  \n",
    "        return self.linear(torch.cat([self.pool(x), meta], dim=1))\n",
    "\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [nn.BatchNorm1d(n_out if lin_first else n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "def create_head(nf, n_out, lin_ftrs=None, ps=0.5, concat_pool=True,\n",
    "                bn_final=False, lin_first=False, y_range=None, actv=None,\n",
    "                relu_fn=nn.ReLU(inplace=True), trial=None, num_lin_ftrs=None, n_lin_ftrs=None, trial_num_lin_ftrs=[1,3],\n",
    "                trial_n_lin_ftrs=[256,512,1024]):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n",
    "    if trial is not None:\n",
    "        lin_ftrs = [nf]\n",
    "        num_lin_ftrs = trial.suggest_int(\"num_lin_ftrs\",\n",
    "                                         trial_num_lin_ftrs[0],\n",
    "                                         trial_num_lin_ftrs[1])\n",
    "        for i in range(num_lin_ftrs):\n",
    "            n_lin_ftrs = trial.suggest_categorical(f\"n_lin_ftrs_{i}\", trial_n_lin_ftrs)\n",
    "            # n_lin_ftrs = trial.suggest_categorical(f\"n_lin_ftrs_{i}\", [1024,1224,1424,1624,1824,2000])\n",
    "            lin_ftrs.append(n_lin_ftrs)\n",
    "        lin_ftrs.append(n_out)\n",
    "    elif num_lin_ftrs is not None and n_lin_ftrs is not None:\n",
    "        lin_ftrs = [nf]\n",
    "        for i in range(num_lin_ftrs):\n",
    "            lin_ftrs.append(n_lin_ftrs[i])\n",
    "        lin_ftrs.append(n_out)\n",
    "    else:\n",
    "        lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "    ps = [ps]\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [relu_fn] * (len(lin_ftrs)-2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    pool_layers = nn.Sequential(*[pool, Flatten()])\n",
    "    layers = []\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    if actv is not None:\n",
    "        layers.append(actv)\n",
    "    # return layers\n",
    "    layers = nn.Sequential(*layers)\n",
    "    return HeadModel(pool=pool_layers, linear=layers)\n",
    "\n",
    "# Initialise the resnet 34 model\n",
    "def create_model(num_classes=4, lin_ftrs=None, ps=0.5, concat_pool=True, bn_final=False, lin_first=False, actv=None,\n",
    "                relu_fn=nn.ReLU(inplace=True), trial=None, num_lin_ftrs=None, n_lin_ftrs=None, trial_num_lin_ftrs=[1,3],\n",
    "                trial_n_lin_ftrs=[256,512,1024]):\n",
    "        body = nn.Sequential(*list(models.get_model(\"resnet34\",weights=\"DEFAULT\").children())[:-2])\n",
    "        head_layers = create_head(1024, num_classes, lin_ftrs, ps, concat_pool, bn_final=False, lin_first=False, actv=None,\n",
    "                relu_fn=nn.ReLU(inplace=True), trial=None, num_lin_ftrs=None, n_lin_ftrs=None, trial_num_lin_ftrs=[1,3],\n",
    "                trial_n_lin_ftrs=[256,512,1024])\n",
    "        return nn.Sequential(body,head_layers)\n",
    "    \n",
    "def load_cols_model(path, device='cpu'):\n",
    "    net = create_model( num_classes=3, lin_ftrs=[512,128,64], ps=0.5)\n",
    "    checkpoint = torch.load(path, map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint, strict=True)\n",
    "    return net.eval()\n",
    "\n",
    "def pdf_to_batch(pdf):\n",
    "    return torch.stack([TF.to_tensor(img) for img in color_fill_pdf_text(pdf)])\n",
    "\n",
    "def pred_cols(pdf, model, classes=[1,2,3], device='cpu'):\n",
    "    model = model.eval().to(device)\n",
    "    batch = pdf_to_batch(pdf)\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(batch)\n",
    "        pred = torch.argmax(y_hat, axis=1) \n",
    "    return [classes[p] for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
